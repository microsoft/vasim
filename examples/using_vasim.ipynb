{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VASIM Autoscaling Simulator Toolkit Example\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. **Applicability:** Integration with various algorithms and parameter customization.\n",
    "\n",
    "2. **Simulation:** Realistic workload modeling, achieved within minutes.\n",
    "\n",
    "3. **Parameter Tuning:** Fine-tuning for optimal performance and cost savings.\n",
    "\n",
    "4. **Cost Analysis:** Demonstrating potential cost savings.\n",
    "\n",
    "\n",
    "### Overview: Autoscaling Components\n",
    "\n",
    "\n",
    "<img src=\"../docs/demo_pics/Autoscaling_infra.png\" alt=\"Autoscaling Components\" title=\"Autoscaling Components\" width=\"300\" >\n",
    "\n",
    "- Application: The running software, such as Postgres and SQL Server.\n",
    "- Controller: Manages tasks and publishes metrics.\n",
    "- Metrics Server: Stores and provides metrics.\n",
    "- Recommender Algorithm: Makes resource allocation decisions.\n",
    "- Scaler Entity: Enacts decisions, adjusting resource allocation.\n",
    "\n",
    "These components work together for effective autoscaling.\n",
    "\n",
    "\n",
    "### Components\n",
    "\n",
    "VASIM is a standalone Python package that internally replicates these components, maintaining and processing scaling states. It also includes a cluster state simulator for current utilization and limits, along with a post-processing analyzer for performance assessment. Input is in the same format as timestamp and utilization datasets. Output is a set of metrics and scaling decision trace.\n",
    "\n",
    "VASIM allows you to try different recommender algorithms to adapt to different workloads. Workloads can be bursty, monotononic, cyclical, and business needs may vary based on complexity, cost, and performance.\n",
    "\n",
    "<img src=\"../docs/demo_pics/VASim_infra.png\" alt=\"VASIM Overview\" title=\"VASIM Overview\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "\n",
    "### Prepare data:\n",
    "Get your test data. For this experiment we'll be working with the [Alibaba dataset](https://github.com/alibaba/clusterdata), trace c_12104.\n",
    "\n",
    "There are some important things to note about the data for Vasim.\n",
    "\n",
    "1. You currently must name your csv's ending with `perf_event_log.csv`. (Ex: `c_12104.csv_perf_event_log.csv`.)  This is so the ingester does not accidentally read in other files in the directory such as output files.\n",
    "2. You must format your data in two columns (`TIMESTAMP`,`CPU_USAGE_ACTUAL`) as follows:  (TODO: We [plan](https://github.com/microsoft/vasim/issues/34) to use Open Telemetry format in the future.) \n",
    "3. You may have multiple CSVs, just put everything in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP,CPU_USAGE_ACTUAL\n",
      "2023.04.02-00:09:00:000,7.2\n",
      "2023.04.02-00:10:00:000,7.04\n",
      "2023.04.02-00:11:00:000,6.88\n",
      "2023.04.02-00:12:00:000,6.72\n",
      "2023.04.02-00:13:00:000,6.48\n",
      "2023.04.02-00:14:00:000,6.501818181818182\n",
      "2023.04.02-00:15:00:000,6.523636363636364\n",
      "2023.04.02-00:16:00:000,6.545454545454546\n",
      "2023.04.02-00:17:00:000,6.567272727272727\n"
     ]
    }
   ],
   "source": [
    "! head data/c_12104.csv_perf_event_log.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare algorithm\n",
    "\n",
    "You also need to implement your Recommender algorithm. There are examples in the [recommender](../recommender) folder.\n",
    "\n",
    "To implement a recommender you will need to create a file for your algorithm, add any paramters you need to the `__init__` function and then implement the core logic of your recommender in the `run()` function.\n",
    "\n",
    "```python\n",
    "class SimpleAdditiveRecommender(Recommender):\n",
    "    def __init__(self, cluster_state_provider, save_metadata=True):\n",
    "        # Copy the code at the top of this function as-is.\n",
    "\n",
    "        # Put your parameters here hard-coded, or pass them in to your\n",
    "        # `metadata.json` file in the `algo_specific_config` section.\n",
    "        self.my_param = self.algo_params.get(\"myparam\", 2)\n",
    "\n",
    "    def run(self, recorded_data):\n",
    "        \"\"\"\n",
    "        This method runs the recommender algorithm and returns the new number of\n",
    "              cores to scale to (new limit).\n",
    "\n",
    "        Inputs:\n",
    "            recorded_data (pd.DataFrame): The recorded metrics data for the current time window to simulate\n",
    "        Returns:\n",
    "            latest_time (datetime): The latest time of the performance data.\n",
    "            new_limit (float): The new number of cores to scale to.\n",
    "        \"\"\"\n",
    "\n",
    "        # Your logic goes here! Look at the data in the `recorded_data` dataframe,\n",
    "        #   do a calculation, and return the number of cores to scale to.\n",
    "\n",
    "        return new_limit\n",
    "```\n",
    "\n",
    "For this example, we'll be working with the [DummyAdditiveRecommender](../recommender/DummyAdditiveRecommender.py), which takes a moving average of the CPU values and adds a fixed buffer amount to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing metadata.json\n",
    "We also need to prepare a file that provides the default set of configuration parameters. You can see some examples of this file in [the test folder](../tests/test_data/alibaba_control_c_29247_denom_1_mini). The default name of this file is `metadata.json`, but you can give it any name and pass it in as a parameter.\n",
    "\n",
    "There are three sets of parameters:\n",
    "* **`algo_specific_config`** : this is where you put the parameters you will use in your algorithm's `run` function\n",
    "  * An example: `addend: 2` for the `DummyAdditiveRecommender` we'll be using for this example.\n",
    "* **`general_config`** : These are the parameters related to how the csv trace data is passed in, and the simulation safe-guards.\n",
    "  * `window` (int): the amount of data that is passed to the algorithm in the `recorded_data` paramaeter in the `run` function. _See \"original window\" below_\n",
    "  * `lag` (float): Number of minutes to wait after making a decision\n",
    "  * `min_cpu_limit` (int): The minimum number of cores to recommend. This is used as a [safety guard](https://github.com/microsoft/vasim/blob/198a06062a91f6455b87710b0e59834530b6ea29/simulator/SimulatedInfraScaler.py#L56) in the simulated infra.\n",
    "  * `max_cpu_limit` (int): The maximum number of cores to recommend. (same as min)\n",
    "* **`prediction_config`** : this relates to the window of [predicted](https://github.com/microsoft/vasim/blob/198a06062a91f6455b87710b0e59834530b6ea29/recommender/cluster_state_provider/PredictiveFileClusterStateProvider.py#L29) data that is fed into the algorithm. It uses a time series to forecast what the data might be in the future to help the algorithm proactively scale\n",
    "  * `waiting_before_predict` (int of minutes) : This is the amount of data to consume before making prediction. It is usually set to `1440`, for 60 min * 24 hours = 1 day.\n",
    "  * `frequency_minutes` (int) : This is how frequent your timestamps are in the csvs you provide. This *MUST* match your csvs.  (Ex: 1 in the above case.) TODO: automate.\n",
    "  * `forecasting_models` (string): For now we only support \"naive\".  This parameter is not currently used because it's the only thing supported.\n",
    "  * `minutes_to_predict` (int): This is the “forecasting horizon”/how much to look forward.  Increasing this means you’ll be MORE proactive in adjusting based on history. This is a good one to tune.\n",
    "  * `total_predictive_window` (int): By [default](https://github.com/microsoft/vasim/blob/198a06062a91f6455b87710b0e59834530b6ea29/recommender/cluster_state_provider/PredictiveFileClusterStateProvider.py#L39), this parameter is `minutes_to_predict`/`frequency_minutes` + `window`.  If you would like to change this, you can do it here. \n",
    "   _This is the \"new window\" in the diagram below_, essentially the total amount of minutes you want \n",
    "\n",
    "\n",
    "Here is a picture that explains the `window` and `prediction_config`:\n",
    "\n",
    "<img src=\"../docs/demo_pics/predictive_window.png\" alt=\"Data Windows\" title=\"Data Windows\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
